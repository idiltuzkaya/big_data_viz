---
title: "Big Data Visualization"
author: "Selin Aytan , Idil Bukre Tuzkaya"
date: "2025-07-28"
output:
  html_document:
    toc: true
    css: style.css
    code_folding: show
  pdf_document:
    toc: true
    toc_depth: '3'
editor_options: 
  markdown: 
    wrap: 72
encoding: UTF-8
---

```{=html}
<style>
body {
  font-family: "Times New Roman", Times, serif;
  font-size: 16px;
  line-height: 1.6;
}
p, li {
  text-align: justify;
}
h1 {
  font-family: "Times New Roman", Times, serif;
  font-size: 2em;
  font-weight: bold;
}
h2 {
  font-family: "Times New Roman", Times, serif;
  font-size: 1.6em;
  font-weight: bold;
}
h3 {
  font-family: "Times New Roman", Times, serif;
  font-size: 1.3em;
  font-weight: normal;
}
h4, h5, h6 {
  font-family: "Times New Roman", Times, serif;
  font-weight: bold;
}
</style>
```

## **PREFACE**

### **Motivation and Goal**

This booklet was born out of a growing need to communicate complex data
in a clear and accessible way. As data grows in volume and complexity,
visual tools have become essential for uncovering insights that might
otherwise remain hidden.

Motivated by a personal interest in data visualization, this project
also aims to fill the gaps left by existing resources—many of which
focus either too heavily on code or abstract theory. Our goal is to
provide a guide that is technically sound yet simple and intuitive,
aimed at readers with a basic understanding of data.

While various tools exist for visualization, this booklet focuses
primarily on R, emphasizing its strengths in flexibility,
reproducibility, and statistical integration. We also briefly compare R
to other tools where relevant, to highlight why it was chosen and how it
supports effective data storytelling from preparation to final
presentation.

### **Challenges and Solutions During the Project**

Throughout the project, we encountered several challenges—both technical
and conceptual. One of the first difficulties was handling large and
complex datasets efficiently within the R environment. Some of the
packages required for big data visualization demanded careful memory
management and unfamiliar workflows, especially when working with tools
like SparkR and Arrow.

Another major challenge was choosing the right type of visualization for
different kinds of data and ensuring that the graphics remained both
accurate and interpretable. It often took multiple iterations to find
the right balance between simplicity and completeness.

We also spent considerable time understanding the underlying structure
of the data and preparing it for analysis. Missing values, inconsistent
formats, and merging different data sources required a lot of manual
adjustment and trial-and-error.

Despite these challenges, we approached each problem
methodically—consulting documentation, testing alternative solutions,
and learning from examples and community discussions. These experiences
not only strengthened our understanding of data visualization in R, but
also improved our overall problem-solving skills in working with
real-world data.

### **Target Audience**

This booklet is intended for students, early-stage researchers, and data
enthusiasts who want to strengthen their understanding of data
visualization using R. It assumes only a basic familiarity with data
analysis and aims to present both core principles and practical examples
in a clear and accessible way, making it useful for both beginners and
those looking to refresh their skills.

### **Thanks**

This project was carried out as part of a summer internship at the
University of Edinburgh, supported by the Erasmus+ programme. We would
like to thank Ozan Evkaya, PhD for his valuable guidance and support
throughout the process. His feedback and expertise greatly contributed
to the development of this booklet.

# *TABLE OF CONTENT*

## **1.Introduction**

### ***1.1 Purpose and Objectives of the Study***

In today's world, with the explosive growth of data, deriving valuable
insights and presenting them in a meaningful way has become ever more
essential. Particularly, when working with big data, statistical
analysis alone is often insufficient; supporting these analyses with
powerful visualizations should be regarded as key priority in
data-driven decision making.

This project aims to explore how classic visualization techniques are
applied to "small" to "moderate" datasets to the realm of big data. Our
project aims to demonstrate that data visualization is not merely about
creating graphs, but about generating meaningful insights. It explores
which types of graphs are suitable for different data structures, how to
construct them using small to moderate datasets, and how these
techniques applied when it comes to big data. Furthermore, the
interactive dashboards and visualizations we used in this booklet
targets foster engagement with reader. By doing so, we aim to contribute
open-source materials and support R community. Main focuses of the
project are:

-   Improving practical skills while dealing with big data.

-   Utilizing R tools such as Spark, Arrow, ggplot2, and tmap in order
    to create scalable and effective visual representations.

-   Highlighting the differences between different types of data, and
    appyling proper visualizations for each.

-   Safeguarding that visualizations are accessible and interpretable
    for both technical and general users.

-   Promoting scientific transparency by designing reproducible analysis
    process.

Through this project, we intend to bridge the gap between big data and
data visualization, illustrating how technical tools in data science can
be used for powerful representations.

### ***1.2 Overview of Datasets Used***

In this Project ... datasets are used. Here is the overview:

**Scottish Index of Multiple Deprivation 2020:** The Scottish Index of
Multiple Deprivation is a relative measure of deprivation across 6,976
small areas (called data zones). If an area is identified as ‘deprived’,
this can relate to people having a low income but it can also mean fewer
resources or opportunities. SIMD looks at the extent to which an area is
deprived across seven domains: income, employment, education, health,
access to services, crime and housing. The dataset description was
obtained from the Scottish Government's [Official SIMD
Documentation](https://www.gov.scot/collections/scottish-index-of-multiple-deprivation-2020/)

**Events Data:**

### ***1.3 Project Roadmap***

This project follows a systematic approach: starting from data
comprehension and preparation, moving on to creating effective
visualizations, and finally scaling them to a big data context while
ensuring reproducibility. Below is a an overview of the major stages:

**1. Data Exploration, Understanding, and Preprocessing**

In this step, EDA(Exploratory Data Analysis) was performed to gain a
solid understanding of the datasets' fundamental characteristics. This
process involved handling missing values, assessing normality, and
identifying data types. In order to apply various visualization
techniques, we deliberately focused on datasets containing diverse data
types. Main focus was primarily on Scotland based data, such as
deprivation and Fringe datasets. In the end, the data was cleaned and
prepared for visualization.

**2.Initial Visualizations with Small/Moderate Datasets**

For this step, we followed an iterative approach to decide which type of
visualizations were suitable for each dataset in terms of aesthetics and
readability. Testing these techniques initially on smaller datasets
provided remarkable insights into how appropriate they scale in context
of big data.

**3.Scaling Visualizations to Big Data**

**4.Spatial and Interactive Visualizations**

**5.Documentation and Reproducibility**

**6.Final Outputs and Contribution**

## **2.R Environment and Package Ecosystem**

### 2.1 Why R for Data Visualization?

R is considered to be one of the most powerful tools and has a gained
significant audience for a variety of factors ranging from its
flexibility to rich package ecosystem to built-in integration of
statistical computing. ıt'S features also include very efficient data
handling and storage facilities. R boasts a comprehensive ecosystem of
packages designed spesifically for data visualization.R is also open
sourced and encompasses a large and active community of both users and
developers. r also integrates quite well with other programming
languages like Python and SQL. it also aloows for embedding of R outputs
in web applications through frameworks like ,making it easier to share
visualizations in interactive web dashboards or other software
applications.In academic and professional settings, the ability to
reproduce analyses is crucial. R scripts can be shared and rerun,
providing an auditable trail of how visualizations were created. This is
particularly important for transparency in research and reporting and
lastly being an open-source program platform ,R is run without any
cost.This makes it a cost-effective solution for data visualization, as
there are no licensing fees involved, making it accessible for students,
researchers, and businesses alike

### 2.2 Core Features of R We Utilized

### 2.3 Overview of Key Packages

-   **Data Preparation:** Raw datasets were received in various formats.
    those datasets were imported by `readxl`which has no external
    dependencies making it easy to install and use on all operating
    systems. Main transformations were handled by `dplyr`using its
    **mutate()**, **filter()**, **summarize()** functions to keep the
    dataset concise. Reshaping and handling of missing data was handled
    by `tdyr`. when datasets grew bigger a switch was made to
    `data.table` to prioritize speed and memory efficency. Combining
    functions such as **fread()**, **fwrite()**, impacted the speed of
    operation greatly.[BURAYA DAHA EKLENİR]. Meanwhile `arrow` allows R
    to read and write columnar Parquet/ Feather files with minimal
    memory use loading only the columns requested through a
    `dplyr`compatible interface. This provides compression,
    multi-threaded I/O, and seamless interoperability with Python,
    Spark, and cloud storage all without changing the analyst’s R
    syntax.

-   **Exploratory data analysis:** Base summary functions and **apply
    family** were mainly used `lapply`, `sapply`, and `vapply`to name a
    few to deliver loop free computations to improve speed across
    multiple variables.

-   **Static and multivariate visualisation:** all fixed graphics were
    produced by `ggplot2`its grammer of grpahics and facet system
    covered bar charts, histograms, boxplots, scatterplots, heatmaps,
    and pair plots.

-   **Spatial Analytics:** Vector geometries were read and manipulated
    using `sf` package which stores points, lines and polygons as
    list-columns inside asn ordinary data frame. since geometries behave
    like any other columns they can be filtered, mutated and joined with
    `dplyr`. The package `tmap` added on to the `sf`package to produce
    thematic maps with a layer syntax resembling the `ggplot2`package.
    **tm_Shape()** identifies the spatial object, **tm_fill()**

[chatten aldım] We employed **tm_polygons()** to produce a choropleth of
deprivation zones, classifying values by quintile for immediate
comparison across the region. Transport corridors were over-laid with
**tm_lines()**, giving readers an infrastructural frame of reference,
while event locations were plotted via \*\*tm_dots() and scaled with
tm_symbols() to reflect venue density and attendance. Continuous
elevation data, imported as a raster layer, were displayed beneath these
vectors with tm_raster() to provide topographical context. Layout
refinements—legends, margins, scale bars, and a north arrow—were applied
through tm_layout(), tm_scale_bar(), and tm_compass(), yielding
publication-ready static output. For stakeholder workshops the identical
script was switched to interactive mode by issuing tmap_mode("view"),
thereby converting the map to a Leaflet widget that allowed participants
to pan, zoom, and query features in real time without additional code.

-   **Interactive Reporting:** `plotly` converted ggplot objects into
    zoomable, hover-enabled widgets. Full dashboards were assembled in
    Shiny, whose reactive model lets user inputs update outputs without
    additional JavaScript. When data lived on a Spark cluster,
    `sparklyr` exposed the same `dplyr` verbs, enabling Shiny panels to
    query distributed storage on demand.

Together, these packages provide a seamless toolchain: data flow from
ingestion and cleaning through exploratory statistics, static and
interactive graphics, spatial mapping, and finally to a web dashboard—
all within a single, reproducible R environment.

### 2.4 Integration with Big Data Tools

To meet two opposing requirements—interactive analysis on a laptop and
batch processing over terabyte-scale tables—this project employs a
tiered architecture: Hadoop supplies reliable cluster storage and
coarse-grain cleansing, Arrow delivers high-throughput columnar
interchange to desktop R, and SparkR provides distributed, in-memory
analytics when data volumes exceed local resources. Each layer therefore
addresses a specific constraint (fault tolerance, I/O bandwidth, or
compute parallelism) while exposing an R interface that preserves a
single, reproducible code base.

[ BEN BUNLARI HİÇ KULLANAMDIĞIM İÇİN BİLMİYORUM] \* **Hadoop for
Durable, Cluster-Wide Storage** All raw inputs beyond local-disk
capacity are ingested into the Hadoop Distributed File System (HDFS),
whose block replication and fault tolerance guarantee persistence across
the compute cluster. Initial tokenisation and outlier removal are
implemented as MapReduce jobs orchestrated by YARN through the rhadoop
interface, keeping heavy file-level transformations close to the data
and minimising network traffic. Cleansed outputs are written back to
HDFS as partitioned text sets that serve as the authoritative source for
later stages. \* **Arrow for Columnar Interchange and Selective
Retrieval** Intermediate tables are serialised from HDFS to compressed
Parquet using Apache Arrow’s native writer, then accessed in R via the
arrow package. The open_dataset() interface exposes these partitions as
lazy Arrow Datasets, enabling filter(), select(), and summarise()
operations to run in Arrow’s multithreaded C++ engine and stream only
requested columns into memory. This columnar workflow provides
compression, high-bandwidth I/O, and language-agnostic interoperability
without altering the analyst’s dplyr syntax.

-   **SparkR for In-Memory Distributed Analytics** When full data
    volumes exceed workstation resources, the pipeline attaches to the
    institutional Apache Spark cluster through SparkR. Parquet files
    produced under Arrow are registered as Spark DataFrames; the
    familiar dplyr verbs are translated into Spark SQL and executed in
    parallel across worker nodes. Model artefacts needed for
    visualisation return to the driver as compact R objects, whereas
    voluminous prediction tables remain distributed on the cluster for
    downstream consumption.

By uniting Hadoop’s fault-tolerant storage, Arrow’s high-throughput
columnar access, and SparkR’s distributed in-memory processing, the same
R codebase scales effortlessly from laptop-sized prototypes to
terabyte-level production workloads, offering rapid iteration in
development and robust performance in deployment.

### 2.5 R Environment Setup

### Wrap-up

## **3. Fundamental Concepts of Data Visualization**

### 3.1 What is Data Visulization,Why Do We Need to Visualize?

### 3.2 Principles of Effective Visualization

### 3.3 Data Types and Appropriate Visualization Techniques

### Wrap-up

## **4. Pipeline & Data Preparation**

### ***4.1 Importance of Data Preparation***

Data preparation is one of the most crucial steps for visualization. Raw
data is often not suitable for further analysis. It may contain missing
values, inappropriate data formats or variables that do not follow a
normal distribution. Visualizing non-prepared data can yield us to
misleading visualizations and wrong outcomes. Therefore, preparing data
at first is an essential step, since proper preparation not only
improves the quality but also increases transparency of visualizations.
In this chapter, we will discuss how to prepare data in R environment
for different sizes of datasets.

### ***4.2 Preparation Process in R***

**For Multiple Deprivation Dataset:**

Format dönüştürdük, numerik yaptık...

**Fringe Dataset**
numerous formats were changed

firslty date columns and Performers #,lowest full price" was found to habe a lot of extreme values. winsorization was appliedage categories were reorginezed and drpooed the "+" sign.
number of performers were unlikely to be more than 100 so they were dropped as well. "lowest concession price" was found to have ıver 50% empty so it was filled with median but skewness rate and kurtosis was too high even after log transformations. it was decided to not use this clomun


```{r}

library(readxl)
data_path <- "data//events_data-2025-01-15-16-07-31.xlsx"
file.exists(data_path)
fringe_data <- read_xlsx(data_path, skip = 2)

```



Performances #, Lowest full price, Lowest concession pricewere changed into there dates and numeric respectfully. dates were converted using lubridate package. added the new performance_days_total column. 
```{r}
library(dplyr)

numeric_version <- c("Performers #",
                      "Performances #",
                      "Lowest full price",
                      "Lowest concession price")  


fringe_data <- fringe_data %>%
  mutate(across(all_of(numeric_version), as.numeric))


fringe_data$`Last performance date` <- as.Date(fringe_data$`Last performance date`)
fringe_data$`First performance date`<- as.Date(fringe_data$`First performance date`)

fringe_data<- fringe_data %>%
  mutate(performance_days_total=as.integer((`Last performance date`-  `First performance date`+1)))
names(fringe_data)


```



fringe festival is 30 days long duration longer than are elimenated.
```{r}  

fringe_data%>%
  filter(performance_days_total < 31)



```

```{r}

library(dplyr)
library(e1071)
library(rstatix)
library(tidyr)
library(ggplot2)



#??
clean_fringe <- fringe_data %>%
  
  mutate(
    age_number=as.numeric(gsub("\\+", "", `Age category`))
  )%>%
  mutate(
    age_group=case_when(
      age_number==0 ~ "All ages",
      age_number<8 ~ "Kids",
      age_number>=8 & age_number<13 ~ "Pre Teens",
      age_number>= 13 & age_number<18 ~ "Teens",
      age_number>=18 ~ "Adults"
      ) ) %>%
  mutate(
    age_group = factor(age_group,
                       levels = c("All ages", "Kids", "Pre Teens", "Teens", "Adults"))) %>% 
  mutate(`Performers #`<100)
  
  
  

skewness(fringe_data$`Lowest concession price`)
fringe_data %>%
  mutate(`Lowest concession price` = replace_na(`Lowest concession price`, median(`Lowest concession price`, na.rm =  T)))

summary(fringe_data$`Lowest concession price`)
glimpse(fringe_data$`Lowest concession price`)
hist(fringe_data$`Lowest concession price`)


fringe_data <- fringe_data %>%
  mutate(`Lowest concession price` = replace_na(`Lowest concession price`, median(`Lowest concession price`, na.rm =  T)))
ggplot(fringe_data, aes( fringe_data$`Lowest concession price`))+
  geom_histogram(bins = 30)

ggplot(fringe_data, aes( log(fringe_data$`Lowest concession price`)))+
  geom_histogram(bins = 30)

kurtosis(fringe_data$`Lowest concession price`)
#çok yüksek değerler geldi. 241 diyor


```


```{r}
library(rstatix)
library(dplyr)


outliers <- fringe_data %>%
  identify_outliers(performance_days_total)


names(outliers)

outliers %>% 
  select(performance_days_total)  


```

### ***4.3 Normality Assesment***

**For Multiple Deprivation Dataset:**

This dataset contains 6976 observations. This number is quite big so
tests like Shapiro Wilk Normality or Lilliefors Test may give
inconsistent results, since these tests become sensitive to any small
change as sample size increases. So the most valid option to asses
normality here is drawing QQ-Plots of the variables we plan to use.

```{r echo=TRUE, collapse=TRUE}

library(readxl)
data_path <- "data//simd2020_withgeog//simd2020_withinds.xlsx"
simd_data <- read_excel(data_path, sheet = "Data")

par(mfrow = c(2, 3))

qqnorm(simd_data$income_rate, col = "black", pch = 20, cex = 0.5, main = "QQ Plot- Income Rate"); 
qqline(simd_data$income_rate, col = "red", lwd = 2)

qqnorm(simd_data$employment_rate, col = "black", pch= 20, cex = 0.5, main = "QQ Plot - Employment Rate"); 
qqline(simd_data$employment_rate, col = "red", lwd = 2)

qqnorm(simd_data$DEPRESS, col = "black", pch = 20, cex = 0.5, main = "QQ Plot - Depression"); 
qqline(simd_data$DEPRESS, col = "red", lwd = 2)

qqnorm(simd_data$EMERG, col = "black", pch = 20, cex = 0.5, main = "QQ Plot - Emergency Stays"); 
qqline(simd_data$EMERG, col = "red", lwd = 2)

qqnorm(simd_data$overcrowded_rate, col = "black", pch=20, cex = 0.5, main = "QQ Plot - Overcrowd Rate"); 
qqline(simd_data$overcrowded_rate, col = "red", lwd = 2)

qqnorm(simd_data$broadband, col = "black", pch = 20, cex = 0.5, main = "QQ Plot- Broadband"); 
qqline(simd_data$broadband, col = "red", lwd = 2)
```

### 4.4 Challenges and Decisions

### Wrap-up

## **5. Exploratory Data Analysis (EDA)**

### 5.1 Summary Statistics

## **6. Visualizations**

### **6.1 Static Visualizations**

### 6.1.1 Bar Charts
```{r}
library(dplyr)
library(ggplot2)

clean_fringe %>% 
  arrange(desc(`Performances #`)) %>%        
  slice_head(n = 20) %>%                     
  ggplot(aes(x = `Performances #`,
             y = reorder(Title, `Performances #`))) +
  geom_col() +                               
  labs(x = "Performances #",
       y = "Title (Top 20)",
       title = "Top 20 most Performing acts in fringe") +
  theme_minimal()
```



```{r}
library(grid)
library(gridExtra)
plot1<- ggplot(clean_fringe)+
  geom_histogram(aes(x= `Performances #`),
                 fill= "blue", bins = 20, color= "black")+
  theme_classic()

plot2 <- ggplot(clean_fringe)+
  geom_histogram(aes(x= `Performers #`),
               fill= "pink", bins = 20, color= "black")+
  theme_classic()

grid.arrange(plot1, plot2, ncol= 2)

```

```{r}
cap_perf       <- quantile(clean_fringe$`Performances #`, .99, na.rm = TRUE)
cap_performers <- quantile(clean_fringe$`Performers #`,   .99, na.rm = TRUE)

df2 <- clean_fringe %>%
  filter(`Performances #` <= cap_perf,
         `Performers #`   <= cap_performers)

p1 <- ggplot(df2, aes(x = `Performances #`)) +
  geom_histogram(binwidth = 1, fill = "royalblue", color = "black") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  labs(x = "Performances #", y = "Count") +
  theme_classic()

p2 <- ggplot(df2, aes(x = `Performers #`)) +
  geom_histogram(binwidth = 1, fill = "pink", color = "black") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  labs(x = "Performers #", y = "Count") +
  theme_classic()

grid.arrange(p1, p2, ncol=2)
#bence güzel dğil bu

```


### 6.1.2 Histograms


```{r}

head(clean_fringe$`Lowest concession price`)
summary(clean_fringe$`Lowest full price`)

ggplot(clean_fringe)+
  geom_histogram(aes(x= `Lowest full price`),
                 fill= "purple", alpha= 0.5, color= "black", bins= 30)+
  geom_histogram(aes(x= `Lowest concession price`),
                 fill= "orange", alpha= 0.5, color= "black", bins=30)+
  labs(x= "lowest price",
       y= "lowest concession price")+
  xlim(c(1,25))+
  theme_minimal()


```


```{r}
ggplot(clean_fringe)+
  geom_histogram(aes(x= performance_days_total),
                 fill= "darkgreen", alpha =0.6, bins = 30, color= "black")+
  labs(title = "total days of performance",,
       x= "performance days",
       y= "count")+
  theme_minimal()

```


### 6.1.3 Boxplots

```{r}

sum(which(is.na(clean_fringe$`Artist type`)))
library(ggplot2)
ggplot(data= clean_fringe, aes(x= `Performances #`, y= `Artist type`))+
  geom_boxplot(outlier.colour = "red", outlier.shape = 21, fill = "lightblue") +
  labs(x = "Number of Performances", y = "Artist type #",
       title = "Number of Performances ~ Artist types") +
  theme_minimal()

```
 



### 6.1.4 Scatterplots

```{r}

library(ggplot2)
ggplot(clean_fringe, aes(x= `Lowest full price`, y= `Performances #`))+
  geom_point(size= 2 , alpha= 0.7)+
  labs(x= "Lowest full price",
       y= "Number of Performances",
       title = "Lowest full price ~ Number of Performances")+
  theme_minimal()
 
qqplot(clean_fringe$`Lowest full price`, clean_fringe$`Performances #`) 
```

### **6.2 Multivariate Visualizations**

### 6.2.1 Faceting

### 6.2.2 Heatmaps/ Treemaps

```{r echo=FALSE, collapse=TRUE}

library(readxl)
library(treemap)
library(RColorBrewer)
library(viridis)

clean_fringe <- read_xlsx("data//clean_fringe.xlsx")


#clean_fringe$`Performances #` <- as.numeric(clean_fringe$`Performances #`)

clean_fringe$Genre <- as.character(clean_fringe$Genre)

clean_fringe$`Artist type` <- as.character(clean_fringe$`Artist type`)

treemap(clean_fringeis, 
        index = c("Artist type", "Genre"),
        vSize = "Performances #",
        vColor = "Artist type",
        type = "categorical" ,
        title = "total performance by genre and artist type",
        palette = viridis(4, option = "D"),
        border.lwds = c(1,1),
        bg.labels = 0,
        border.col = "black",
        fontsize.labels = c(0,7),
        fontcolor.labels =c(1,1),
        fontfamily.labels = "sans",
        fontface.labels = c(3,2),
        align.labels = list(c("center", "center"), c("center","center")),
        inflate.labels = F,
        position.legend = "right",
        fontsize.legend = 6)



```

### 6.2.3 Pair plots

### **6.3 Themes, Labels, Scales**

### **6.4 Statistical Analysis and Visualizations**

### 6.4.1 Distribution & Density Plots

```{r}
ggplot(clean_fringe, aes(x=Genre, y= `Lowest full price`))+
  geom_violin(fill= "#b2df8a", alpha=0.7, color= "black")+
  geom_boxplot(width=0.125, fill= "#fdd0a2", outlier.colour = "red")+
  labs(title= "Lowest price by genre",
       x= "Genre",
       y= "Lowest Price in pound")
  theme_light()
```



```{r}

ggplot(clean_fring, aes(x=Genre, y= performance_days_total))+
  geom_violin(fill= "lightpink", alpha= 0.7, color = "black")+
  geom_boxplot(width= 0.1, fill= "#cbb67c", outlier.colour = "red")+
  labs(title= " Distirbution of total days of performance by genre",
       x= "Genre",
       y= "Total days of performance")+
  theme_minimal()
```

```{r}
ggplot(clean_fringe, aes(x= age_group, y= `Performances #`))+
  geom_violin(fill= "blue", color="black", alpha= 0.4)+
  geom_boxplot(width=0.4, fill= "lightblue", outlier.color= "red")+
  labs(title= "  # perofrmances by age categories",
       x= "age categories",
       y= "# of performance")+
  theme_minimal()
```


### 6.4.2 Linear and Multiple Regression

### 6.4.3 Confidence Intervals and Comparisons

### 6.4.4 Box Plots for Statistical Comparisons

### 6.4.5 ANOVA

### Wrap-up

## **7. Visualizing Geographical Patterns**

### 7.1 Understanding Spatial Data Types(vektör bitmap farkı)

### 7.2 Merging Spatial and Non-Spatial Data

### 7.3 Mapping Tools and Packages in R

### 7.4 Thematic Mapping and Color Scales

### 7.5 Best Practices in Spatial Visualizations

### Wrap-up

## **8. Interactive Visualizations**

### 8.1 Introduction to Interactivity in R

### 8.2 Building Interactive Plots with Plotly

### 8.3 Building Interactive Dashboards with Shiny

### Integrating Maps into Interactive Visualizations

### Considerations and Limitations

### Wrap-up

## **9. Findings and Insights**

### 9.1 Key Visual Patterns

### 9.2 Interpretation of Statistical Results

### 9.3 Challenges and Reflections

## **10. Conclusion**

### 10.1 Summary of Findings

### 10.2 Contributions and Future Work

## **REFERENCES**

## **APPENDIX**
